{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Point Forecast Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lecture you will learn:**\n",
    "    \n",
    "* How to partition your time series data into training and test sets\n",
    "* The definition of a point forecast error\n",
    "* The difference between scale dependent and relative error measures\n",
    "* How to compute *mean absolute error*\n",
    "* How to compute *mean absolute percentage error*\n",
    "* The difference between in-sample and out-of-sample error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.style as style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for this lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_month = pd.read_csv('data/ed_mth_ts.csv', index_col='date', parse_dates=True)\n",
    "ed_month.index.freq='MS'\n",
    "arrival_rate = ed_month['arrivals'] / ed_month.index.days_in_month\n",
    "arrival_rate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Train-Test Split\n",
    "\n",
    "Just like in 'standard' machine learning problems it is important to seperate the data used for model training and model testing.  A key difference with time series forecasting is that you must take the temporal ordering of data into account. \n",
    "\n",
    "The good news is that pandas makes train test split of data very simple.  There are two options:\n",
    "\n",
    "1. Split the dataframe using `DataFrame.iloc[start:end]` \n",
    "2. Split the dataframe using dates.\n",
    "\n",
    "**Method 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_rate.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = arrival_rate.shape[0] - 12\n",
    "train, test = arrival_rate.iloc[:train_length], arrival_rate.iloc[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATE = '2016-06-01'\n",
    "train = arrival_rate.loc[arrival_rate.index < SPLIT_DATE]\n",
    "test = arrival_rate.loc[arrival_rate.index >= SPLIT_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT - DO NOT LOOK AT THE TEST SET!\n",
    "\n",
    "We need to **hold back** a proportion of our data.  This is so we can simulate real forecasting conditions and check a models accuracy on **unseen** data.  We don't want to know what it looks like as that will introduce bias into the forecasting process and mean we overfit our model to the data we hold.\n",
    "\n",
    "**Remember - there is no such thing as real time data from the future!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train.plot(figsize=(12,4))\n",
    "ax.set_ylabel('ed arrivals')\n",
    "ax.legend(['training data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Forecasts\n",
    "\n",
    "The numbers we produced using the baseline methods in the last lecture are called **point forecasts**.  They are actually the mean value of a **forecast distribution**.  As a reminder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast.baseline import SNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf = SNaive(period=12)\n",
    "snf.fit(train)\n",
    "preds = snf.predict(horizon=12)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in `preds` are point forecasts.  For the time being we will focus on point forecasts.  We will revisit forecast distributions in a future lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Forecast Errors\n",
    "\n",
    "The point forecast is our best estimate of future observations of the time series.  We use our test set (some times called a holdout set) to simulate real world forecasting.  As our forecasting method has not seen this data before we can measure the difference between the forecast and the ground-truth observed value.  \n",
    "\n",
    "**Problem: Errors can be both positive and negative so just taking the average will mask the true size of the errors.**  \n",
    "\n",
    "* There are a large number of forecast error metrics available.  Each has its own pro's and con's.  Here we review some of the most used in practice.\n",
    "\n",
    "### MAE and MSE\n",
    "\n",
    "* A simple way to remedy the problem with the average error is to use **Mean Absolute Error (MAE)** or **Mean Squared Error (MSE)**.  \n",
    "* There's a bit of a debate about if you should take the median value or the mean, but here we will just use the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true=test, y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=test, y_pred=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean absolute error is conceptually easier to understand than MSE. \n",
    "    * The dimensions of MSE are airpassengers squared!  Which is odd!  \n",
    "* One way to remedy this units issue is the **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "RMSE = $\\sqrt{mean(e_t^2)}$ where $e_t$ is the error in predicting $y_t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_true=test, y_pred=preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE and MAE are called 'scale dependent' measures as the units and magnitude are specific to the problem and context.  An alternative approach is to use a scale invariant measure such as the **mean absolute percentage error (MAPE)**\n",
    "\n",
    "The percentage error is given by $p_t = \\frac{100e_t}{y_t}$ where $e_t$ is the error in predicting $y_t$.  \n",
    "\n",
    "Therefore, MAPE = $mean(|p_t|)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=test, y_pred=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of MAPE is that it is inflated when the denominator is small relative to the absolute forecast error (such in the case of outliers or extreme unexpected events). It is also penalises negative errors more than positive errors.  A consequence of this property is that MAPE can lead to selecting a model that tends to under forecast.  The two following examples illustrate the issue. $$APE_{1} = \\left| \\frac{y_t - \\hat{y_t}}{y_t} \\right|= \\left| \\frac{150 - 100}{150} \\right| = \\frac{50}{150} = 33.33\\%$$  \n",
    "\n",
    "$$APE_{2} = \\left| \\frac{100 - 150}{100} \\right| = \\frac{50}{100} = 50\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the difference between in-sample and out-of-sample error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample errors\n",
    "\n",
    "These errors are often called the models **residuals** they represent the difference between the training data (the data the model has seen) and the models fitted values.  For example, let's a look at the residuals of the SNaive model fitted to the ED arrival data and then calculate the in-sample MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf._fitted.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the predictions via the `fittedvalues` property.  Notice that the first 12 observations to not have a prediction.  This is because of the way SNaive works i.e. carrying forward the previous 12 observations.  You cannot carry forward observations that do not exist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And access the residuals via the `resid` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=train[12:], y_pred=snf.fittedvalues[12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of sample errors\n",
    "\n",
    "* Out of sample errors are based on predictions of observations the model has not seen (in the test set).  \n",
    "* These are the point forecast errors we have already calculated.  \n",
    "* You should expect the out-of-sample errors to be larger than in-sample errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=test, y_pred=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing forecasting methods using a test (holdout) set.\n",
    "\n",
    "Let's compare the MAE of the methods on the ED dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convenience function for creating all objects quickly\n",
    "from forecast.baseline import baseline_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = baseline_estimators(seasonal_periods=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = len(test)\n",
    "\n",
    "print(f'{HORIZON}-Step MAE\\n----------')\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train)\n",
    "    preds = model.predict(HORIZON)\n",
    "    mae = mean_absolute_error(y_true=test, y_pred=preds)\n",
    "    print(f'{model_name}: {mae:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
