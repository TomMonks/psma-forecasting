{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till now we have used a single validation period to select our best model.  The weakness of that approach is that it gives you a sample size of 1 (that's better than nothing, but generally poor statistics!).  Time series cross validation is an approach to provide more data points when comparing models. In the classicial time series literature time series cross validation is called a **Rolling Forecast Origin**.  There may also be benefit of taking a **sliding window** approach to cross validaiton.  This second approach maintains a fixed sized training set.  I.e. it drops older values from the time series during validation.\n",
    "\n",
    "## Rolling Forecast Origin\n",
    "\n",
    "The following code and output provide a simplified view of how rolling forecast horizons work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_forecast_origin(train, min_train_size, horizon):\n",
    "    '''\n",
    "    Rolling forecast origin generator.\n",
    "    '''\n",
    "    for i in range(len(train) - min_train_size - horizon + 1):\n",
    "        split_train = train[:min_train_size+i]\n",
    "        split_val = train[min_train_size+i:min_train_size+i+horizon]\n",
    "        yield split_train, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_series = [2502, 2414, 2800, 2143, 2708, 1900, 2333, 2222, 1234, 3456]\n",
    "\n",
    "test = full_series[-2:]\n",
    "train = full_series[:-2]\n",
    "print('full training set: {0}'.format(train))\n",
    "print('hidden test set: {0}'.format(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rolling = rolling_forecast_origin(train, min_train_size=4, horizon=2)\n",
    "cv_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for cv_train, cv_val in cv_rolling:\n",
    "    print(f'CV[{i+1}]')\n",
    "    print(f'Train:\\t{cv_train}')\n",
    "    print(f'Val:\\t{cv_val}')\n",
    "    print('-----')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size, horizon):\n",
    "    '''\n",
    "    sliding window  generator.\n",
    "    '''\n",
    "    for i in range(len(train) - window_size - horizon + 1):\n",
    "        split_train = train[i:window_size+i]\n",
    "        split_val = train[i+window_size:window_size+i+horizon]\n",
    "        yield split_train, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sliding = sliding_window(train, window_size=4, horizon=1)\n",
    "\n",
    "print('full training set: {0}\\n'.format(train))\n",
    "\n",
    "i = 0\n",
    "for cv_train, cv_val in cv_sliding:\n",
    "    print(f'CV[{i+1}]')\n",
    "    print(f'Train:\\t{cv_train}')\n",
    "    print(f'Val:\\t{cv_val}')\n",
    "    print('-----')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(model, train, cv, metric):\n",
    "    '''\n",
    "    Calculate cross validation scores\n",
    "    '''\n",
    "    cv_scores = []\n",
    "    for cv_train, cv_test in cv:\n",
    "        model.fit(cv_train)\n",
    "        preds = model.predict(horizon=len(cv_test))\n",
    "        score=metric(y_true=cv_test, y_pred=preds)\n",
    "        cv_scores.append(score)\n",
    "    return np.array(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast.baseline import SNaive, Naive1\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the ED attendance dataset\n",
    "ed_month = pd.read_csv('data/ed_mth_ts.csv', index_col='date', parse_dates=True)\n",
    "ed_month.index.freq='MS'\n",
    "arrival_rate = ed_month['arrivals'] / ed_month.index.days_in_month\n",
    "arrival_rate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATE = '2016-06-01'\n",
    "train = arrival_rate.loc[arrival_rate.index < SPLIT_DATE]\n",
    "test = arrival_rate.loc[arrival_rate.index >= SPLIT_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sliding = sliding_window(train, window_size=24, horizon=12)\n",
    "\n",
    "cv_scores_1 = cross_validation_score(model=SNaive(period=12), \n",
    "                                   train=train, \n",
    "                                   cv=cv_sliding, \n",
    "                                   metric=mean_absolute_error)\n",
    "\n",
    "pd.DataFrame(cv_scores_1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sliding = sliding_window(train, window_size=24, horizon=12)\n",
    "\n",
    "cv_scores_2 = cross_validation_score(model=Naive1(), \n",
    "                                   train=train, \n",
    "                                   cv=cv_sliding, \n",
    "                                   metric=mean_absolute_error)\n",
    "\n",
    "pd.DataFrame(cv_scores_2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(cv_scores_1).plot(figsize=(12,4))\n",
    "pd.DataFrame(cv_scores_2).plot(ax=ax)\n",
    "ax.legend(['SNaive', 'Naive1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SNaive(period=12)\n",
    "model.fit(train)\n",
    "preds = model.predict(horizon=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
